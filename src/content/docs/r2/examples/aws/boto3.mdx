---
title: boto3
pcx_content_type: example
---

import { Render } from "~/components";

<Render file="keys" />
<br />

You must configure [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to use a preconstructed `endpoint_url` value. This can be done through any `boto3` usage that accepts connection arguments; for example:

:::note[Compatibility]
Client version `1.36.0` introduced a modification to the default checksum behavior from the client that is currently incompatible with R2 APIs.

To mitigate, users can use `1.35.99` or add the following to their s3 resource config:

```python
request_checksum_calculation = 'when_required',
response_checksum_validation = 'when_required'
```

If checksum is needed, R2 supports `SHA-1` and `SHA-256` algorithms.
:::

```python
import boto3

s3 = boto3.resource('s3',
  endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
  aws_access_key_id = '<access_key_id>',
  aws_secret_access_key = '<access_key_secret>'
)
```

You may, however, omit the `aws_access_key_id` and `aws_secret_access_key ` arguments and allow `boto3` to rely on the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` [environment variables](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables) instead.

An example script may look like the following:

```python
import boto3

s3 = boto3.client(
    service_name ="s3",
    endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
    aws_access_key_id = '<access_key_id>',
    aws_secret_access_key = '<access_key_secret>',
    region_name="<location>", # Must be one of: wnam, enam, weur, eeur, apac, auto
)

# Get object information
object_information = s3.head_object(Bucket=<R2_BUCKET_NAME>, Key=<FILE_KEY_NAME>)

# Upload/Update single file
s3.upload_fileobj(io.BytesIO(file_content), <R2_BUCKET_NAME>, <FILE_KEY_NAME>)

# Delete object
s3.delete_object(Bucket=<R2_BUCKET_NAME>, Key=<FILE_KEY_NAME>)
```

```sh
python main.py
```

```sh output
Buckets:
 -  user-uploads
 -  my-bucket-name
Objects:
 -  cat.png
 -  todos.txt
```

### With SHA-1/SHA-256 checksum algorithms

You can also use SHA-1 and SHA-256 algorithms for checksum.

```python
import boto3
import hashlib
import base64

def calculate_sha1_base64(data):
    """Calculate SHA-1 hash and return base64 encoded string."""
    sha1_hash = hashlib.sha1(data).digest()
    return base64.b64encode(sha1_hash).decode('utf-8')

def calculate_sha256_base64(data):
    """Calculate SHA-256 hash and return base64 encoded string."""
    sha256_hash = hashlib.sha256(data).digest()
    return base64.b64encode(sha256_hash).decode('utf-8')

s3 = boto3.client(
    service_name ="s3",
    endpoint_url = 'https://<accountid>.r2.cloudflarestorage.com',
    aws_access_key_id = '<access_key_id>',
    aws_secret_access_key = '<access_key_secret>',
    region_name="<location>", # Must be one of: wnam, enam, weur, eeur, apac, auto
)

# Calculate SHA1
sha1 = calculate_sha1_base64(file_data)

# Uplodad file to R2 with SHA1 checksum
response = s3.put_object(
  Body=file_data,
  Bucket=bucket_name,
  Key='sha1.txt',
  ChecksumSHA1=sha1)

# Calculate SHA256
sha256 = calculate_sha256_base64(file_data)

# Uplodad file to R2 with SHA256 checksum
response = s3.put_object(
  Body=file_data,
  Bucket=bucket_name,
  Key='sha256.txt',
  ChecksumSHA256=sha256)
```
